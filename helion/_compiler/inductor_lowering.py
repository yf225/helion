from __future__ import annotations

import ast
import dataclasses
import functools
from operator import getitem
from typing import TYPE_CHECKING
from typing import Callable
from typing import NamedTuple

import sympy
import torch
from torch._dynamo.convert_frame import compile_lock
from torch._inductor import config as inductor_config
from torch._inductor.codegen.simd import SIMDKernelFeatures
from torch._inductor.codegen.triton import TritonKernel
from torch._inductor.codegen.triton import TritonOverrides
from torch._inductor.graph import GraphLowering
from torch._inductor.ir import ComputedBuffer
from torch._inductor.ir import FixedLayout
from torch._inductor.ir import InputBuffer
from torch._inductor.ir import Pointwise
from torch._inductor.ir import Reduction
from torch._inductor.ir import StorageBox
from torch._inductor.ir import TensorBox
from torch._inductor.ops_handler import DefaultHandler
from torch._inductor.virtualized import OpsValue
from torch._inductor.virtualized import V
from torch.fx.interpreter import Interpreter
from torch.fx.node import Node
from torch.fx.node import map_arg

from .._compat import min_dot_size
from ..exc import InductorLoweringError
from ..language._decorators import APIFunc
from ..language._decorators import is_api_func
from .ast_extension import create
from .ast_extension import expr_from_string
from .ast_extension import statement_from_string
from .compile_environment import CompileEnvironment
from .tile_strategy import TileStrategy
from .tile_strategy import TileStrategyDispatch

if TYPE_CHECKING:
    from .. import Config
    from .device_function import DeviceFunction
    from .generate_ast import GenerateAST

    CodegenHandler = Callable[["GraphInterpreter", torch.fx.Node], object]


def prepare_graph_lowerings(gm: torch.fx.GraphModule) -> None:
    with compile_lock:
        graph_lowering = GraphLowering(
            gm, shape_env=CompileEnvironment.current().shape_env
        )
        # pyre-ignore[19]
        with V.set_graph_handler(graph_lowering):
            for node in gm.graph.nodes:
                assert node.op in {
                    "call_function",
                    "placeholder",
                    "output",
                }, node.op
                if node.op == "call_function":
                    prior_buffers = len(graph_lowering.buffers)
                    node.meta["lowering"] = prepare_node_lowering(graph_lowering, node)
                    if len(graph_lowering.buffers) > prior_buffers + 1:
                        raise InductorLoweringError(
                            f"Lowering {node.op} resulted in {len(graph_lowering.buffers) - prior_buffers} buffers, expected 1."
                        )


def prepare_node_lowering(
    graph_lowering: GraphLowering,
    node: Node,
) -> Lowering:
    if is_api_func(api := node.target):
        APIFuncLowering.normalize_args_kwargs(api, node)
        return APIFuncLowering(api)

    if node.target in aten_lowering_dispatch:
        return aten_lowering_dispatch[node.target](node)

    def convert_arg(arg: Node) -> TensorBox:
        example = arg.meta["val"]
        assert isinstance(example, torch.Tensor), (
            f"Expected Tensor, got {type(example)}: {node.target}"
        )
        input_names.append(name := f"{node.name}_input{len(input_names)}")
        return TensorBox.create(
            InputBuffer(
                name=name,
                layout=FixedLayout(
                    example.device,
                    example.dtype,
                    [*map(_unpack_symint, example.size())],
                    [*map(_unpack_symint, example.stride())],
                ),
            )
        )

    input_names: list[str] = []
    result = graph_lowering.call_function(
        # pyre-ignore[6]
        node.target,
        # pyre-ignore[6]
        *map_arg((node.args, node.kwargs), convert_arg),
    )
    result.realize()
    if not isinstance(result, TensorBox) or not isinstance(result.data, StorageBox):
        raise InductorLoweringError(
            f"Lowering {node.target} returned type(result), expected TensorBox(StorageBox(...)): {result}"
        )
    if not isinstance(buffer := result.data.data, ComputedBuffer):
        raise InductorLoweringError(
            f"Lowering {node.target} returned buffer type {type(buffer)}, expected ComputedBuffer: {buffer}"
        )
    if isinstance(buffer.data, Pointwise):
        return PointwiseLowering(buffer, input_names)
    if isinstance(buffer.data, Reduction):
        return ReductionLowering(buffer, input_names)
    raise InductorLoweringError(
        f"Lowering {node.target} returned buffer type {type(buffer.data)}, expected Pointwise or Reduction: {buffer}"
    )


def _unpack_symint(x: torch.SymInt | int) -> sympy.Expr:
    if isinstance(x, torch.SymInt):
        return x._sympy_()
    if isinstance(x, int):
        return sympy.sympify(x)
    raise TypeError(f"Expected SymInt or int, got {type(x)}")


class Lowering:
    def codegen(self, ctx: GraphInterpreter, node: torch.fx.Node) -> object:
        raise NotImplementedError


@dataclasses.dataclass
class InductorLowering(Lowering):
    buffer: ComputedBuffer
    input_names: list[str]

    def input_asts(self, ctx: GraphInterpreter, node: torch.fx.Node) -> list[ast.AST]:
        input_asts: list[ast.AST] = []
        map_arg(
            (node.args, node.kwargs),
            lambda arg: input_asts.append(ctx.env[arg]),
        )
        assert len(input_asts) == len(self.input_names)
        return input_asts

    def codegen(self, ctx: GraphInterpreter, node: torch.fx.Node) -> object:
        raise NotImplementedError(
            f"codegen not implemented for {type(self).__name__}: {self.buffer}"
        )


@functools.cache
def dummy_gm() -> torch.fx.GraphModule:
    return torch.fx.symbolic_trace(lambda: None)


class PointwiseLowering(InductorLowering):
    def codegen(self, ctx: GraphInterpreter, node: torch.fx.Node) -> object:
        with (
            inductor_config.patch("triton.codegen_upcast_to_fp32", False),
            # pyre-ignore[19]
            V.set_graph_handler(
                GraphLowering(
                    dummy_gm(), shape_env=CompileEnvironment.current().shape_env
                )
            ),
            # pyre-ignore[19]
            V.set_ops_handler(
                GenerateASTFromInductor(
                    ctx.cg, dict(zip(self.input_names, self.input_asts(ctx, node)))
                )
            ),
            # pyre-ignore[19]
            V.set_kernel_handler(
                TritonKernel({}, features=SIMDKernelFeatures([], sympy.S.One))
            ),
        ):
            indices = [
                sympy.Symbol(f"i{n}") for n in range(len(self.buffer.data.ranges))
            ]
            output_name = _unpack_opsvalue(self.buffer.data.inner_fn(indices))
            return expr_from_string(output_name)


class ReductionLowering(InductorLowering):
    pass


@dataclasses.dataclass
class APIFuncLowering(Lowering):
    api_func: APIFunc

    def codegen(self, ctx: GraphInterpreter, node: torch.fx.Node) -> object:
        assert not node.kwargs
        ast_args = [*map_arg(node.args, lambda arg: ctx.env[arg])]
        proxy_args = [*map_arg(node.args, lambda arg: arg.meta["val"])]

        assert self.api_func._codegen is not None
        return self.api_func._codegen(
            CodegenState(
                ctx.cg,
                fx_node=node,
                # pyre-ignore[6]
                proxy_args=proxy_args,
                # pyre-ignore[6]
                ast_args=ast_args,
            ),
        )

    @staticmethod
    def normalize_args_kwargs(
        api_func: APIFunc,
        node: torch.fx.Node,
    ) -> None:
        bound = api_func._signature.bind(*node.args, **node.kwargs)
        bound.apply_defaults()
        node.args = (*bound.arguments.values(),)
        node.kwargs = {}


@dataclasses.dataclass
class LambdaLowering(Lowering):
    fn: Callable[..., object]

    def codegen(self, ctx: GraphInterpreter, node: torch.fx.Node) -> object:
        return self.fn(ctx, node)


aten_lowering_dispatch: dict[object, Callable[[torch.fx.Node], Lowering]] = {}


def default_make_lowering(handler: CodegenHandler, node: torch.fx.Node) -> Lowering:
    return LambdaLowering(handler)


def register_lowering(
    fn: object,
    make_lowering: Callable[
        [CodegenHandler, torch.fx.Node], Lowering
    ] = default_make_lowering,
) -> Callable[[CodegenHandler], CodegenHandler]:
    def decorator(handler: CodegenHandler) -> CodegenHandler:
        assert fn not in aten_lowering_dispatch, f"Lowering for {fn} already registered"
        aten_lowering_dispatch[fn] = lambda node: make_lowering(handler, node)
        return handler

    return decorator


# pyre-fixme[56]
@register_lowering(torch.ops.aten.sym_size.int)
def codegen_sym_size(ctx: GraphInterpreter, node: torch.fx.Node) -> object:
    val = node.meta["val"]
    assert isinstance(
        val, (int, float, bool, torch.SymInt, torch.SymBool, torch.SymFloat)
    )
    return val


@register_lowering(getitem)
def codegen_getitem(ctx: GraphInterpreter, node: torch.fx.Node) -> object:
    assert not node.kwargs, "getitem kwargs not supported"
    lhs, rhs = map_arg(node.args, lambda arg: ctx.env[arg])
    assert isinstance(lhs, (list, tuple))
    assert isinstance(rhs, int)
    return lhs[rhs]


def apply_dot_requirements(handler: CodegenHandler, node: torch.fx.Node) -> Lowering:
    """Apply min_dot_size requirements to the config_spec"""
    assert not node.kwargs, "dot kwargs not supported"
    assert len(node.args) in (2, 3)
    lproxy, rproxy = map_arg(node.args[-2:], lambda arg: arg.meta["val"])
    assert isinstance(lproxy, torch.Tensor)
    assert isinstance(rproxy, torch.Tensor)
    n, k = lproxy.size()
    _, m = rproxy.size()
    a, b, c = min_dot_size(lproxy.device, lproxy.dtype, rproxy.dtype)
    env = CompileEnvironment.current()
    for shape, min_size in [(n, a), (k, b), (m, c)]:
        block_idx = TileStrategy.get_block_index(shape)
        if block_idx is not None:
            env.config_spec.update_min_block(block_idx, min_size, allow_flattened=False)
    return LambdaLowering(handler)


# pyre-fixme[56]
@register_lowering(torch.ops.aten.mm.default, apply_dot_requirements)
def codegen_mm(ctx: GraphInterpreter, node: torch.fx.Node) -> ast.AST:
    assert not node.kwargs, "matmul kwargs not supported"
    lhs, rhs = map_arg(node.args, lambda arg: ctx.env[arg])
    assert isinstance(lhs, ast.AST)
    assert isinstance(rhs, ast.AST)
    tf32 = CompileEnvironment.current().settings.dot_precision
    return expr_from_string(
        f"tl.dot(lhs, rhs, input_precision={tf32!r})", lhs=lhs, rhs=rhs
    )


# pyre-fixme[56]
@register_lowering(torch.ops.aten.addmm.default, apply_dot_requirements)
def codegen_addmm(ctx: GraphInterpreter, node: torch.fx.Node) -> ast.AST:
    assert not node.kwargs, "addmm kwargs not supported"
    acc, lhs, rhs = map_arg(node.args, lambda arg: ctx.env[arg])
    assert isinstance(acc, ast.AST)
    assert isinstance(lhs, ast.AST)
    assert isinstance(rhs, ast.AST)
    tf32 = CompileEnvironment.current().settings.dot_precision
    return expr_from_string(
        f"tl.dot(lhs, rhs, acc=acc, input_precision={tf32!r})",
        lhs=lhs,
        rhs=rhs,
        acc=acc,
    )


class GenerateASTFromInductor(DefaultHandler):
    def __init__(self, cg: GenerateAST, input_name_lookup: dict[str, ast.AST]) -> None:
        super().__init__()
        self.parent_handler = TritonOverrides()
        self.cg = cg
        self.input_name_lookup = input_name_lookup

    def _default(
        self, name: str, args: tuple[object, ...], kwargs: dict[str, object]
    ) -> str:
        result_str = _unpack_opsvalue(
            getattr(self.parent_handler, name)(*args, **kwargs)
        )
        return self.cg.lift(expr_from_string(result_str)).id

    def load(self, name: str, index: sympy.Expr) -> str:
        # TODO(jansel): assert the index is correct
        return self.cg.lift(self.input_name_lookup[name]).id


def _unpack_opsvalue(value: object) -> str:
    if isinstance(value, OpsValue):
        return str(value)
    assert isinstance(value, str)
    return value


class GraphInterpreter(Interpreter):
    def __init__(self, gm: torch.fx.GraphModule, cg: GenerateAST) -> None:
        super().__init__(gm, garbage_collect_values=False)
        self.cg = cg

    def run_node(self, n: Node) -> object:
        if n.op == "call_function":
            with self._set_current_node(n), n.meta["location"]:
                lowering: Lowering = n.meta["lowering"]
                result = lowering.codegen(self, n)
                if result is None:
                    return None
                if not isinstance(result, ast.AST):
                    return result
                assert isinstance(result, ast.expr)
                if len(n.users) > 0:
                    if isinstance(result, (ast.Name, ast.Constant)):
                        return result
                    name = self.cg.device_function.new_var(n.name)
                    self.cg.add_statement(
                        statement_from_string(f"{name} = result", result=result)
                    )
                    return create(ast.Name, id=name, ctx=ast.Load())
                if not isinstance(result, (ast.Name, ast.Constant)):
                    self.cg.add_statement(create(ast.Expr, value=result))
                return None
        return super().run_node(n)


def codegen_call_with_graph(
    cg: GenerateAST, gm: torch.fx.GraphModule, args: list[ast.AST]
) -> list[object]:
    with compile_lock:
        return GraphInterpreter(gm, cg).run(*args)


class CodegenState(NamedTuple):
    codegen: GenerateAST
    fx_node: torch.fx.Node
    proxy_args: list[object]
    ast_args: list[object]

    def proxy_arg(self, i: int) -> object:
        return self.proxy_args[i]

    def ast_arg(self, i: int) -> ast.AST:
        rv = self.ast_args[i]
        assert isinstance(rv, ast.AST), "TODO: convert nested/defaults"
        return rv

    @property
    def fake_value(self) -> object:
        return self.fx_node.meta["val"]

    @property
    def device_function(self) -> DeviceFunction:
        return self.codegen.device_function

    @property
    def tile_strategy(self) -> TileStrategyDispatch:
        return self.codegen.device_function.tile_strategy

    @property
    def config(self) -> Config:
        return self.codegen.device_function.config

    def add_statement(self, statement: ast.AST | str) -> None:
        return self.codegen.add_statement(statement)
